# my global config
global:
  scrape_interval: 15s # 设定抓取数据的周期，默认为1min
  evaluation_interval: 15s # 设定更新rules文件的周期，默认为1min
 #external_labels: # 额外的属性，会添加到拉取得数据并存到数据库中
 #  monitor: 'codelab_monitor'
  # scrape_timeout is set to the global default (10s).
 
# Alertmanager configuration
alerting:
  alertmanagers:
  - static_configs:
    - targets: ['192.168.52.129:9093'] #设定alertmanager和prometheus交互的接口，即alertmanager监听的ip地址和端口
      # - alertmanager:9093
 
# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
# rule告警规则配置，首次读取默认加载，之后根据evaluation_interval设定的周期加载
rule_files:
  - "alert-node.yml"
  #- "*rules.yml"
  - "*rules.yml"
  # - "second_rules.yml"
 
# A scrape configuration containing exactly one endpoint to scrape:
# Here it's Prometheus itself.
# 配置采集目标endpoints
scrape_configs:
  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
  - job_name: 'prometheus' #job_name默认写入timeseries的labels中，可以用于查询使用
    #scrape_interval: 15s # 抓取周期，默认采用global配置
    static_configs: #静态配置
    - targets: ['192.168.52.129:9090'] # prometheus所要抓取数据的地址，即instance实例项
 


  - job_name: 'node'
    file_sd_configs:
      - files:
        - node.yml


  - job_name: 'cadvisor'
    file_sd_configs:
      - files:
        - cadvisor.yml

